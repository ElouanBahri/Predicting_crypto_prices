{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le Stat ARIMA... (SARIMA)\n",
    "\n",
    "\n",
    "librairie : statmodel (library, package ARIMA)\n",
    "\n",
    "\n",
    "Model LLM, Ollama, TimeCHATGPT...\n",
    "\n",
    "√† compl√©ter et battre le BTC tout seul est quasiment impossible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from modules.utils import feature_engineering_last, filter_data_by_year_month\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\n",
    "    \"/Users/elouan/A. Repo Github ElouanBahri/Predicting_crypto_prices/Historical Prices for BTCUSDT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "Data = filter_data_by_year_month(X, YEARS)\n",
    "\n",
    "df = feature_engineering_last(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"target\"]\n",
    "\n",
    "df_without_target = df.drop(columns=[\"target\"], errors=\"ignore\")  # Only keep features\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(df_without_target)\n",
    "\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = (\n",
    "    np.argmax(explained_variance >= 0.95) + 1\n",
    ")  # Find min components for 95% variance\n",
    "\n",
    "if num_components == 0:\n",
    "    print(\"‚ö†Ô∏è No PCA components meet 95% explained variance. Using 1 component.\")\n",
    "    num_components = 1\n",
    "\n",
    "print(f\"‚úÖ Optimal number of components: {num_components}\")\n",
    "\n",
    "# ‚úÖ Step 2: Fit PCA with optimal number of components\n",
    "pca = PCA(n_components=num_components)\n",
    "X_reduced = pca.fit_transform(df_without_target)\n",
    "\n",
    "# ‚úÖ Convert back to DataFrame and restore index\n",
    "columns = [f\"PC{i+1}\" for i in range(num_components)]\n",
    "X_pca_df = pd.DataFrame(\n",
    "    X_reduced, columns=columns, index=df.index\n",
    ")  # Keep original index\n",
    "\n",
    "X_pca_df[\"target\"] = y  # Add target column back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"target\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the entire DataFrame\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.replace([np.inf, -np.inf], np.nan)  # Replace inf with NaN\n",
    "# df = df.fillna(df.mean())\n",
    "\n",
    "X = X_pca_df.drop(columns=[\"target\"]).values  # Features\n",
    "y = X_pca_df[\"target\"].values  # Target variable\n",
    "\n",
    "\n",
    "# Reshape the data into sequences (timesteps)\n",
    "timesteps = 5  # Number of timesteps for the RNN\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(len(X) - timesteps):\n",
    "    X_sequences.append(X[i : i + timesteps])\n",
    "    y_sequences.append(y[i + timesteps])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "split_index = int(len(X_sequences) * 0.8)  # 80% train, 20% test\n",
    "X_train, X_test = X_sequences[:split_index], X_sequences[split_index:]\n",
    "y_train, y_test = y_sequences[:split_index], y_sequences[split_index:]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(X).any(), np.isnan(y).any())  # Check for NaN\n",
    "print(np.isinf(X).any(), np.isinf(y).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices((X1, y))\n",
    "# # Calculate sizes for train and validation sets\n",
    "# dataset_size = len(X1)  # Total number of samples in your dataset\n",
    "# train_size = int(0.8 * dataset_size)\n",
    "\n",
    "# # Shuffle the dataset if needed\n",
    "# dataset = dataset.shuffle(buffer_size=dataset_size)\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset = dataset.take(train_size)       # Take the first 80%\n",
    "# val_dataset = dataset.skip(train_size)         # Skip the first 80%, take the remaining 20%\n",
    "\n",
    "# # Batch both datasets if needed\n",
    "# batch_size = 32\n",
    "# train_dataset = train_dataset.batch(batch_size)\n",
    "# val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # for consistent results\n",
    "\n",
    "# Simplified GRU Model\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        # Single GRU Layer\n",
    "        GRU(64, input_shape=(10, X_sequences.shape[2]), name=\"GRU_Layer\"),\n",
    "        Dropout(0.2, name=\"Dropout_Layer\"),  # Regularization\n",
    "        # Fully Connected Dense Layer\n",
    "        Dense(32, activation=\"relu\", name=\"Dense_Layer\"),  # Hidden dense layer\n",
    "        # Output Layer for Binary Classification\n",
    "        Dense(1, activation=\"sigmoid\", name=\"Output_Layer\"),  # Predict binary output\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # for consistent results\n",
    "\n",
    "\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Bidirectional(\n",
    "            LSTM(128, return_sequences=True),\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\"),  # Binary classification\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # for consistent results\n",
    "\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Bidirectional(\n",
    "            LSTM(256, return_sequences=True),\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\"),  # Binary classification\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = tf.keras.models.load_model(\"../models/RNN(O.5646)_high(0.55)_low_(0.44).keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model3.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),  # üî• Pour classification binaire\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    factor=0.1,  # Reduce LR by 90% instead of 50%\n",
    "    patience=3,\n",
    "    min_lr=1e-6,  # Set a minimum learning rate\n",
    ")\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"../models/RNN_LTSM_model_00.keras\",  # Save the model to this path\n",
    "    monitor=\"val_accuracy\",  # Monitor validation accuracy\n",
    "    mode=\"max\",  # Save when accuracy is maximized\n",
    "    save_best_only=True,  # Save only if the model improves\n",
    "    verbose=1,  # Display a message when saving\n",
    ")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model3.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,  # Increase epochs for better training\n",
    "    batch_size=32,\n",
    "    callbacks=[reduce_lr, checkpoint, early_stop],  # Add early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Pr√©dictions sur le test set\n",
    "y_pred_prob = model_.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.44).astype(int)  # Convertir en 0 ou 1\n",
    "\n",
    "# Calculer l‚Äôaccuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Afficher un rapport d√©taill√© (Precision, Recall, F1-score)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_test = pd.DataFrame({\"Actual Return\": y_test, \"Predicted\": y_pred.flatten()})\n",
    "df_test[\"Signal\"] = df_test[\"Predicted\"].shift(\n",
    "    1\n",
    ")  # D√©calage pour √©viter la fuite d‚Äôinfo\n",
    "\n",
    "# Appliquer les signaux pour simuler une strat√©gie\n",
    "df_test[\"Strategy Return\"] = df_test[\"Signal\"] * df_test[\"Actual Return\"]\n",
    "df_test[\"Cumulative Return\"] = df_test[\"Strategy Return\"].cumsum()\n",
    "\n",
    "# Afficher la performance de la strat√©gie\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_test[\"Cumulative Return\"], label=\"LSTM Trading Strategy\", color=\"green\")\n",
    "plt.title(\"Backtest: Cumulative Returns of LSTM Strategy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_2D = X_train.reshape(X_train.shape[0], -1)  # Flatten\n",
    "X_test_2D = X_test.reshape(X_test.shape[0], -1)  # Flatten\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_2D, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred_rf = rf.predict(X_test_2D)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "xgb.fit(X_train_2D, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb.predict(X_test_2D)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance\n",
    "importance = xgb.feature_importances_\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(importance)), importance)\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge_ENS_env_CFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
