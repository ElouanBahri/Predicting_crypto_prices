{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from modules.utils import features_engineering\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('/Users/elouan/Repo Github ElouanBahri/Predicting_crypto_prices/Historical Prices for BTCUSDT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEARS = [2019,2021,2022,2023,2024,2025]\n",
    "\n",
    "# Data = filter_data_by_year_month(X, YEARS)\n",
    "\n",
    "df = features_engineering(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `df` contains feature-engineered data, with 'target' as the label column\n",
    "X = df.drop(columns=['target']).values  # Features\n",
    "y = df['target'].values  # Target variable\n",
    "\n",
    "# Reshape the data into sequences (timesteps)\n",
    "timesteps = 10  # Number of timesteps for the RNN\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(len(X) - timesteps):\n",
    "    X_sequences.append(X[i:i+timesteps])\n",
    "    y_sequences.append(y[i+timesteps])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices((X1, y))\n",
    "# # Calculate sizes for train and validation sets\n",
    "# dataset_size = len(X1)  # Total number of samples in your dataset\n",
    "# train_size = int(0.8 * dataset_size)\n",
    "\n",
    "# # Shuffle the dataset if needed\n",
    "# dataset = dataset.shuffle(buffer_size=dataset_size)\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset = dataset.take(train_size)       # Take the first 80%\n",
    "# val_dataset = dataset.skip(train_size)         # Skip the first 80%, take the remaining 20%\n",
    "\n",
    "# # Batch both datasets if needed\n",
    "# batch_size = 32\n",
    "# train_dataset = train_dataset.batch(batch_size)\n",
    "# val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # First GRU Layer\n",
    "    GRU(128, input_shape=(10, X_sequences.shape[2]), return_sequences=True, name=\"GRU_Layer1\"),\n",
    "    Dropout(0.3, name=\"Dropout_Layer1\"),  # Regularization\n",
    "    BatchNormalization(name=\"BatchNorm_Layer1\"),  # Normalize to stabilize training\n",
    "    \n",
    "    # Second GRU Layer\n",
    "    GRU(64, return_sequences=False, name=\"GRU_Layer2\"),\n",
    "    Dropout(0.3, name=\"Dropout_Layer2\"),\n",
    "\n",
    "    # Fully Connected Dense Layers\n",
    "    Dense(128, activation='relu', name=\"Dense_Layer1\"),\n",
    "    Dropout(0.3, name=\"Dropout_Layer3\"),  # Regularization\n",
    "    Dense(64, activation='relu', name=\"Dense_Layer2\"),\n",
    "\n",
    "    # Output Layer for Binary Classification\n",
    "    Dense(1, activation='sigmoid', name=\"Output_Layer\")  # Predicting probability of class 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"../models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),  # Binary classification loss\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),      # Adam optimizer\n",
    "    metrics=['accuracy']                                         # Track accuracy\n",
    ")\n",
    "\n",
    "# Learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    factor=0.5,          # Reduce learning rate by a factor of 0.5\n",
    "    patience=3,          # Wait 3 epochs with no improvement before reducing\n",
    "    min_lr=1e-6          # Set a minimum learning rate\n",
    ")\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../models/RNN_model_2.keras',  # Save the model to this path\n",
    "    monitor='val_accuracy',                 # Monitor validation accuracy\n",
    "    mode='max',                             # Save when accuracy is maximized\n",
    "    save_best_only=True,                    # Save only if the model improves\n",
    "    verbose=1                               # Display a message when saving\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,                         # Training data\n",
    "    validation_data=(X_test, y_test),         # Validation data\n",
    "    epochs=20,                                # Train for 20 epochs\n",
    "    batch_size=32,                            # Batch size\n",
    "    callbacks=[reduce_lr, checkpoint]         # Use the callbacks\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge_ENS_env_CFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
