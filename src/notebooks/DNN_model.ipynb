{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from modules.utils import filter_data_by_year_month, create_features_from_past, create_X_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('/Users/elouan/Repo Github ElouanBahri/Predicting_crypto_prices/Historical Prices for BTCUSDT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [2024]\n",
    "\n",
    "Data = filter_data_by_year_month(X, YEARS)\n",
    "\n",
    "Data1 = create_features_from_past(Data,['close', 'open', 'high', 'low', 'volume'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>lag_close1</th>\n",
       "      <th>lag_open1</th>\n",
       "      <th>lag_high1</th>\n",
       "      <th>lag_low1</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_close3</th>\n",
       "      <th>lag_open3</th>\n",
       "      <th>lag_high3</th>\n",
       "      <th>lag_low3</th>\n",
       "      <th>lag_volume3</th>\n",
       "      <th>lag_close4</th>\n",
       "      <th>lag_open4</th>\n",
       "      <th>lag_high4</th>\n",
       "      <th>lag_low4</th>\n",
       "      <th>lag_volume4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222911</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>188.76099</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>...</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42488.00</td>\n",
       "      <td>42554.57</td>\n",
       "      <td>42412.02</td>\n",
       "      <td>392.24889</td>\n",
       "      <td>42488.00</td>\n",
       "      <td>42283.58</td>\n",
       "      <td>42488.09</td>\n",
       "      <td>42261.02</td>\n",
       "      <td>431.71082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222912</th>\n",
       "      <td>2024-01-01 01:15:00</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42513.17</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>152.48691</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>...</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42447.82</td>\n",
       "      <td>42354.19</td>\n",
       "      <td>319.90644</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42488.00</td>\n",
       "      <td>42554.57</td>\n",
       "      <td>42412.02</td>\n",
       "      <td>392.24889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222913</th>\n",
       "      <td>2024-01-01 01:30:00</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42775.00</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42697.50</td>\n",
       "      <td>642.57307</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42513.17</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>...</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>127.81493</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42447.82</td>\n",
       "      <td>42354.19</td>\n",
       "      <td>319.90644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222914</th>\n",
       "      <td>2024-01-01 01:45:00</td>\n",
       "      <td>42697.50</td>\n",
       "      <td>42720.00</td>\n",
       "      <td>42613.56</td>\n",
       "      <td>42613.56</td>\n",
       "      <td>212.55759</td>\n",
       "      <td>42697.50</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42775.00</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>...</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>188.76099</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>127.81493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222915</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>42613.57</td>\n",
       "      <td>42638.41</td>\n",
       "      <td>42541.10</td>\n",
       "      <td>42556.63</td>\n",
       "      <td>217.29821</td>\n",
       "      <td>42613.56</td>\n",
       "      <td>42697.50</td>\n",
       "      <td>42720.00</td>\n",
       "      <td>42613.56</td>\n",
       "      <td>...</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42513.17</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>152.48691</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>188.76099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp      open      high       low     close     volume  \\\n",
       "222911 2024-01-01 01:00:00  42475.23  42475.23  42431.65  42466.33  188.76099   \n",
       "222912 2024-01-01 01:15:00  42466.32  42513.17  42466.32  42493.16  152.48691   \n",
       "222913 2024-01-01 01:30:00  42493.16  42775.00  42493.16  42697.50  642.57307   \n",
       "222914 2024-01-01 01:45:00  42697.50  42720.00  42613.56  42613.56  212.55759   \n",
       "222915 2024-01-01 02:00:00  42613.57  42638.41  42541.10  42556.63  217.29821   \n",
       "\n",
       "        lag_close1  lag_open1  lag_high1  lag_low1  ...  lag_close3  \\\n",
       "222911    42475.23   42441.32   42490.74  42422.45  ...    42419.73   \n",
       "222912    42466.33   42475.23   42475.23  42431.65  ...    42441.32   \n",
       "222913    42493.16   42466.32   42513.17  42466.32  ...    42475.23   \n",
       "222914    42697.50   42493.16   42775.00  42493.16  ...    42466.33   \n",
       "222915    42613.56   42697.50   42720.00  42613.56  ...    42493.16   \n",
       "\n",
       "        lag_open3  lag_high3  lag_low3  lag_volume3  lag_close4  lag_open4  \\\n",
       "222911   42488.00   42554.57  42412.02    392.24889    42488.00   42283.58   \n",
       "222912   42419.73   42447.82  42354.19    319.90644    42419.73   42488.00   \n",
       "222913   42441.32   42490.74  42422.45    127.81493    42441.32   42419.73   \n",
       "222914   42475.23   42475.23  42431.65    188.76099    42475.23   42441.32   \n",
       "222915   42466.32   42513.17  42466.32    152.48691    42466.33   42475.23   \n",
       "\n",
       "        lag_high4  lag_low4  lag_volume4  \n",
       "222911   42488.09  42261.02    431.71082  \n",
       "222912   42554.57  42412.02    392.24889  \n",
       "222913   42447.82  42354.19    319.90644  \n",
       "222914   42490.74  42422.45    127.81493  \n",
       "222915   42475.23  42431.65    188.76099  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nan_rows = Data1.isna().any(axis=1).sum()\n",
    "print(num_nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = create_X_y(Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag_close1</th>\n",
       "      <th>lag_open1</th>\n",
       "      <th>lag_high1</th>\n",
       "      <th>lag_low1</th>\n",
       "      <th>lag_volume1</th>\n",
       "      <th>lag_close2</th>\n",
       "      <th>lag_open2</th>\n",
       "      <th>lag_high2</th>\n",
       "      <th>lag_low2</th>\n",
       "      <th>lag_volume2</th>\n",
       "      <th>lag_close3</th>\n",
       "      <th>lag_open3</th>\n",
       "      <th>lag_high3</th>\n",
       "      <th>lag_low3</th>\n",
       "      <th>lag_volume3</th>\n",
       "      <th>lag_close4</th>\n",
       "      <th>lag_open4</th>\n",
       "      <th>lag_high4</th>\n",
       "      <th>lag_low4</th>\n",
       "      <th>lag_volume4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222911</th>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>127.81493</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42447.82</td>\n",
       "      <td>42354.19</td>\n",
       "      <td>319.90644</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42488.00</td>\n",
       "      <td>42554.57</td>\n",
       "      <td>42412.02</td>\n",
       "      <td>392.24889</td>\n",
       "      <td>42488.00</td>\n",
       "      <td>42283.58</td>\n",
       "      <td>42488.09</td>\n",
       "      <td>42261.02</td>\n",
       "      <td>431.71082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222912</th>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>188.76099</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>127.81493</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42447.82</td>\n",
       "      <td>42354.19</td>\n",
       "      <td>319.90644</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42488.00</td>\n",
       "      <td>42554.57</td>\n",
       "      <td>42412.02</td>\n",
       "      <td>392.24889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222913</th>\n",
       "      <td>42493.16</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42513.17</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>152.48691</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>188.76099</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>127.81493</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42419.73</td>\n",
       "      <td>42447.82</td>\n",
       "      <td>42354.19</td>\n",
       "      <td>319.90644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222914</th>\n",
       "      <td>42697.50</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42775.00</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>642.57307</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42513.17</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>152.48691</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>188.76099</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42441.32</td>\n",
       "      <td>42490.74</td>\n",
       "      <td>42422.45</td>\n",
       "      <td>127.81493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222915</th>\n",
       "      <td>42613.56</td>\n",
       "      <td>42697.50</td>\n",
       "      <td>42720.00</td>\n",
       "      <td>42613.56</td>\n",
       "      <td>212.55759</td>\n",
       "      <td>42697.50</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42775.00</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>642.57307</td>\n",
       "      <td>42493.16</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>42513.17</td>\n",
       "      <td>42466.32</td>\n",
       "      <td>152.48691</td>\n",
       "      <td>42466.33</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42475.23</td>\n",
       "      <td>42431.65</td>\n",
       "      <td>188.76099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lag_close1  lag_open1  lag_high1  lag_low1  lag_volume1  lag_close2  \\\n",
       "222911    42475.23   42441.32   42490.74  42422.45    127.81493    42441.32   \n",
       "222912    42466.33   42475.23   42475.23  42431.65    188.76099    42475.23   \n",
       "222913    42493.16   42466.32   42513.17  42466.32    152.48691    42466.33   \n",
       "222914    42697.50   42493.16   42775.00  42493.16    642.57307    42493.16   \n",
       "222915    42613.56   42697.50   42720.00  42613.56    212.55759    42697.50   \n",
       "\n",
       "        lag_open2  lag_high2  lag_low2  lag_volume2  lag_close3  lag_open3  \\\n",
       "222911   42419.73   42447.82  42354.19    319.90644    42419.73   42488.00   \n",
       "222912   42441.32   42490.74  42422.45    127.81493    42441.32   42419.73   \n",
       "222913   42475.23   42475.23  42431.65    188.76099    42475.23   42441.32   \n",
       "222914   42466.32   42513.17  42466.32    152.48691    42466.33   42475.23   \n",
       "222915   42493.16   42775.00  42493.16    642.57307    42493.16   42466.32   \n",
       "\n",
       "        lag_high3  lag_low3  lag_volume3  lag_close4  lag_open4  lag_high4  \\\n",
       "222911   42554.57  42412.02    392.24889    42488.00   42283.58   42488.09   \n",
       "222912   42447.82  42354.19    319.90644    42419.73   42488.00   42554.57   \n",
       "222913   42490.74  42422.45    127.81493    42441.32   42419.73   42447.82   \n",
       "222914   42475.23  42431.65    188.76099    42475.23   42441.32   42490.74   \n",
       "222915   42513.17  42466.32    152.48691    42466.33   42475.23   42475.23   \n",
       "\n",
       "        lag_low4  lag_volume4  \n",
       "222911  42261.02    431.71082  \n",
       "222912  42412.02    392.24889  \n",
       "222913  42354.19    319.90644  \n",
       "222914  42422.45    127.81493  \n",
       "222915  42431.65    188.76099  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 35132 entries, 222911 to 258042\n",
      "Data columns (total 20 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   lag_close1   35132 non-null  float64\n",
      " 1   lag_open1    35132 non-null  float64\n",
      " 2   lag_high1    35132 non-null  float64\n",
      " 3   lag_low1     35132 non-null  float64\n",
      " 4   lag_volume1  35132 non-null  float64\n",
      " 5   lag_close2   35132 non-null  float64\n",
      " 6   lag_open2    35132 non-null  float64\n",
      " 7   lag_high2    35132 non-null  float64\n",
      " 8   lag_low2     35132 non-null  float64\n",
      " 9   lag_volume2  35132 non-null  float64\n",
      " 10  lag_close3   35132 non-null  float64\n",
      " 11  lag_open3    35132 non-null  float64\n",
      " 12  lag_high3    35132 non-null  float64\n",
      " 13  lag_low3     35132 non-null  float64\n",
      " 14  lag_volume3  35132 non-null  float64\n",
      " 15  lag_close4   35132 non-null  float64\n",
      " 16  lag_open4    35132 non-null  float64\n",
      " 17  lag_high4    35132 non-null  float64\n",
      " 18  lag_low4     35132 non-null  float64\n",
      " 19  lag_volume4  35132 non-null  float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 5.6 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sizes for train and validation sets\n",
    "dataset_size = len(X)  # Total number of samples in your dataset\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "# Shuffle the dataset if needed\n",
    "dataset = dataset.shuffle(buffer_size=dataset_size)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset.take(train_size)       # Take the first 80%\n",
    "val_dataset = dataset.skip(train_size)         # Skip the first 80%, take the remaining 20%\n",
    "\n",
    "# Batch both datasets if needed\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(16, activation='relu', input_shape=(20,), name=\"Input_Layer\"),  # Input layer with 21 features\n",
    "        Dropout(0.3),\n",
    "        Dense(8, activation='relu', name=\"Hidden_Layer1\"),  # Another hidden layer\n",
    "        Dense(1, activation='linear', name=\"Output_Layer\")  # Output layer for regression\n",
    "    ],\n",
    "    name=\"Price_Prediction_Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"../models/model_1.h5\") #IF you need to dowload a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 212157872.0000 - mae: 10059.0820 - val_loss: 218609952.0000 - val_mae: 10239.2998 - learning_rate: 0.1000\n",
      "Epoch 2/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 216117936.0000 - mae: 10174.5820 - val_loss: 219153952.0000 - val_mae: 10359.4717 - learning_rate: 0.1000\n",
      "Epoch 3/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855us/step - loss: 214656864.0000 - mae: 10157.0635 - val_loss: 211154672.0000 - val_mae: 10076.2070 - learning_rate: 0.1000\n",
      "Epoch 4/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 878us/step - loss: 216507776.0000 - mae: 10233.3291 - val_loss: 218344272.0000 - val_mae: 10190.1279 - learning_rate: 0.1000\n",
      "Epoch 5/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 219021792.0000 - mae: 10288.2939 - val_loss: 223274240.0000 - val_mae: 10366.2354 - learning_rate: 0.1000\n",
      "Epoch 6/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888us/step - loss: 215774720.0000 - mae: 10170.3594 - val_loss: 214110192.0000 - val_mae: 10161.5576 - learning_rate: 0.1000\n",
      "Epoch 7/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 216595280.0000 - mae: 10200.0781 - val_loss: 214474064.0000 - val_mae: 10128.5244 - learning_rate: 0.1000\n",
      "Epoch 8/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - loss: 213937216.0000 - mae: 10126.3340 - val_loss: 217320144.0000 - val_mae: 10127.7197 - learning_rate: 0.1000\n",
      "Epoch 9/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 218237632.0000 - mae: 10240.3672 - val_loss: 212895504.0000 - val_mae: 10069.5625 - learning_rate: 0.1000\n",
      "Epoch 10/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 212364496.0000 - mae: 10053.1045 - val_loss: 216460336.0000 - val_mae: 10194.4229 - learning_rate: 0.0500\n",
      "Epoch 11/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 218896624.0000 - mae: 10290.3613 - val_loss: 221879136.0000 - val_mae: 10283.7480 - learning_rate: 0.0500\n",
      "Epoch 12/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 869us/step - loss: 216899232.0000 - mae: 10205.1445 - val_loss: 216925824.0000 - val_mae: 10255.4648 - learning_rate: 0.0500\n",
      "Epoch 13/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 942us/step - loss: 217386560.0000 - mae: 10243.3564 - val_loss: 218520112.0000 - val_mae: 10217.4209 - learning_rate: 0.0500\n",
      "Epoch 14/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 215461376.0000 - mae: 10176.7754 - val_loss: 215259472.0000 - val_mae: 10125.9395 - learning_rate: 0.0500\n",
      "Epoch 15/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 215355488.0000 - mae: 10154.0361 - val_loss: 211060432.0000 - val_mae: 10052.0566 - learning_rate: 0.0500\n",
      "Epoch 16/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 217127200.0000 - mae: 10212.3311 - val_loss: 214602176.0000 - val_mae: 10161.4717 - learning_rate: 0.0500\n",
      "Epoch 17/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 215301232.0000 - mae: 10174.8271 - val_loss: 216277312.0000 - val_mae: 10227.6309 - learning_rate: 0.0500\n",
      "Epoch 18/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 214643120.0000 - mae: 10132.6211 - val_loss: 217880336.0000 - val_mae: 10285.2207 - learning_rate: 0.0500\n",
      "Epoch 19/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871us/step - loss: 211127856.0000 - mae: 10013.2451 - val_loss: 215016096.0000 - val_mae: 10109.5059 - learning_rate: 0.0500\n",
      "Epoch 20/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 211564864.0000 - mae: 10061.2520 - val_loss: 220547136.0000 - val_mae: 10388.8213 - learning_rate: 0.0500\n",
      "Epoch 21/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - loss: 216035504.0000 - mae: 10160.2666 - val_loss: 214939984.0000 - val_mae: 10077.3330 - learning_rate: 0.0500\n",
      "Epoch 22/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 935us/step - loss: 216994432.0000 - mae: 10196.8438 - val_loss: 216539616.0000 - val_mae: 10224.3994 - learning_rate: 0.0250\n",
      "Epoch 23/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 215298720.0000 - mae: 10189.8906 - val_loss: 213313616.0000 - val_mae: 10136.4277 - learning_rate: 0.0250\n",
      "Epoch 24/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 215926112.0000 - mae: 10194.2725 - val_loss: 219298880.0000 - val_mae: 10317.3486 - learning_rate: 0.0250\n",
      "Epoch 25/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 218294800.0000 - mae: 10268.6738 - val_loss: 215392544.0000 - val_mae: 10143.1846 - learning_rate: 0.0250\n",
      "Epoch 26/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 215942624.0000 - mae: 10199.5137 - val_loss: 214766096.0000 - val_mae: 10088.2412 - learning_rate: 0.0250\n",
      "Epoch 27/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step - loss: 216489472.0000 - mae: 10191.0010 - val_loss: 209893120.0000 - val_mae: 10000.6729 - learning_rate: 0.0250\n",
      "Epoch 28/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 215212176.0000 - mae: 10157.9180 - val_loss: 213047328.0000 - val_mae: 10145.9873 - learning_rate: 0.0250\n",
      "Epoch 29/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 217194288.0000 - mae: 10233.4609 - val_loss: 214456992.0000 - val_mae: 10086.4990 - learning_rate: 0.0250\n",
      "Epoch 30/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 216297888.0000 - mae: 10208.0518 - val_loss: 216960032.0000 - val_mae: 10267.6338 - learning_rate: 0.0250\n",
      "Epoch 31/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891us/step - loss: 212221616.0000 - mae: 10078.5303 - val_loss: 211740800.0000 - val_mae: 10103.1484 - learning_rate: 0.0250\n",
      "Epoch 32/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - loss: 213852928.0000 - mae: 10107.6211 - val_loss: 220848960.0000 - val_mae: 10381.0820 - learning_rate: 0.0250\n",
      "Epoch 33/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 213836160.0000 - mae: 10122.4717 - val_loss: 210814512.0000 - val_mae: 9989.0088 - learning_rate: 0.0250\n",
      "Epoch 34/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 214333984.0000 - mae: 10133.2334 - val_loss: 215580848.0000 - val_mae: 10163.2041 - learning_rate: 0.0125\n",
      "Epoch 35/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step - loss: 218589280.0000 - mae: 10249.5479 - val_loss: 219549696.0000 - val_mae: 10302.5801 - learning_rate: 0.0125\n",
      "Epoch 36/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step - loss: 214833968.0000 - mae: 10145.4463 - val_loss: 216926544.0000 - val_mae: 10161.6777 - learning_rate: 0.0125\n",
      "Epoch 37/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 215792432.0000 - mae: 10141.6416 - val_loss: 217790576.0000 - val_mae: 10147.1670 - learning_rate: 0.0125\n",
      "Epoch 38/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 212333968.0000 - mae: 10066.7529 - val_loss: 210628736.0000 - val_mae: 10025.1592 - learning_rate: 0.0125\n",
      "Epoch 39/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896us/step - loss: 215756656.0000 - mae: 10181.0557 - val_loss: 212357216.0000 - val_mae: 10086.3545 - learning_rate: 0.0125\n",
      "Epoch 40/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 216166784.0000 - mae: 10167.5088 - val_loss: 215972256.0000 - val_mae: 10190.4912 - learning_rate: 0.0063\n",
      "Epoch 41/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 216421520.0000 - mae: 10182.2920 - val_loss: 214538912.0000 - val_mae: 10125.3906 - learning_rate: 0.0063\n",
      "Epoch 42/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 216993184.0000 - mae: 10252.7461 - val_loss: 215688000.0000 - val_mae: 10136.4971 - learning_rate: 0.0063\n",
      "Epoch 43/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 218202144.0000 - mae: 10245.0254 - val_loss: 214444944.0000 - val_mae: 10137.3311 - learning_rate: 0.0063\n",
      "Epoch 44/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - loss: 217551600.0000 - mae: 10207.9209 - val_loss: 214174992.0000 - val_mae: 10116.5439 - learning_rate: 0.0063\n",
      "Epoch 45/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 906us/step - loss: 221098752.0000 - mae: 10359.0732 - val_loss: 212224960.0000 - val_mae: 10074.0244 - learning_rate: 0.0063\n",
      "Epoch 46/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 216650064.0000 - mae: 10212.0244 - val_loss: 219492736.0000 - val_mae: 10276.3037 - learning_rate: 0.0031\n",
      "Epoch 47/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 214223984.0000 - mae: 10139.7314 - val_loss: 212042560.0000 - val_mae: 10118.9014 - learning_rate: 0.0031\n",
      "Epoch 48/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - loss: 216885296.0000 - mae: 10176.6094 - val_loss: 211880176.0000 - val_mae: 10071.7549 - learning_rate: 0.0031\n",
      "Epoch 49/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 211259120.0000 - mae: 10026.3887 - val_loss: 215425392.0000 - val_mae: 10113.4189 - learning_rate: 0.0031\n",
      "Epoch 50/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - loss: 210210640.0000 - mae: 10016.6162 - val_loss: 219562144.0000 - val_mae: 10258.5840 - learning_rate: 0.0031\n",
      "Epoch 51/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 215750128.0000 - mae: 10172.3203 - val_loss: 213963248.0000 - val_mae: 10131.4062 - learning_rate: 0.0031\n",
      "Epoch 52/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 928us/step - loss: 219191552.0000 - mae: 10291.2695 - val_loss: 216525664.0000 - val_mae: 10225.9971 - learning_rate: 0.0016\n",
      "Epoch 53/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 215958048.0000 - mae: 10140.7998 - val_loss: 216329088.0000 - val_mae: 10216.5205 - learning_rate: 0.0016\n",
      "Epoch 54/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step - loss: 213670704.0000 - mae: 10105.6357 - val_loss: 210913824.0000 - val_mae: 10077.6543 - learning_rate: 0.0016\n",
      "Epoch 55/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 212280272.0000 - mae: 10087.0088 - val_loss: 218639584.0000 - val_mae: 10296.9668 - learning_rate: 0.0016\n",
      "Epoch 56/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982us/step - loss: 218016560.0000 - mae: 10251.1807 - val_loss: 215667760.0000 - val_mae: 10174.6904 - learning_rate: 0.0016\n",
      "Epoch 57/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 933us/step - loss: 210837824.0000 - mae: 10033.6963 - val_loss: 208621888.0000 - val_mae: 9930.4795 - learning_rate: 0.0016\n",
      "Epoch 58/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855us/step - loss: 219399632.0000 - mae: 10275.8770 - val_loss: 219996832.0000 - val_mae: 10278.6123 - learning_rate: 0.0016\n",
      "Epoch 59/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 218409920.0000 - mae: 10228.8545 - val_loss: 218553920.0000 - val_mae: 10239.9336 - learning_rate: 0.0016\n",
      "Epoch 60/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - loss: 219168176.0000 - mae: 10285.8145 - val_loss: 214936448.0000 - val_mae: 10146.9521 - learning_rate: 0.0016\n",
      "Epoch 61/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 214222976.0000 - mae: 10140.4277 - val_loss: 216063376.0000 - val_mae: 10202.0049 - learning_rate: 0.0016\n",
      "Epoch 62/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853us/step - loss: 216693536.0000 - mae: 10200.4912 - val_loss: 214875408.0000 - val_mae: 10122.4238 - learning_rate: 0.0016\n",
      "Epoch 63/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step - loss: 214182848.0000 - mae: 10115.2891 - val_loss: 217629216.0000 - val_mae: 10224.0840 - learning_rate: 0.0016\n",
      "Epoch 64/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 214608352.0000 - mae: 10154.8545 - val_loss: 221551248.0000 - val_mae: 10281.1670 - learning_rate: 7.8125e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step - loss: 218095056.0000 - mae: 10205.8857 - val_loss: 214507568.0000 - val_mae: 10069.9268 - learning_rate: 7.8125e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 215043984.0000 - mae: 10181.7109 - val_loss: 209747824.0000 - val_mae: 9959.0312 - learning_rate: 7.8125e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 849us/step - loss: 215513472.0000 - mae: 10162.3818 - val_loss: 216670832.0000 - val_mae: 10160.0186 - learning_rate: 7.8125e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - loss: 216180704.0000 - mae: 10162.7979 - val_loss: 218504896.0000 - val_mae: 10287.8105 - learning_rate: 7.8125e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 217159744.0000 - mae: 10214.0684 - val_loss: 218049824.0000 - val_mae: 10207.0518 - learning_rate: 7.8125e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 213998416.0000 - mae: 10118.4531 - val_loss: 209973936.0000 - val_mae: 10022.1826 - learning_rate: 3.9063e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 210742816.0000 - mae: 10022.1709 - val_loss: 216934144.0000 - val_mae: 10227.5449 - learning_rate: 3.9063e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 210887408.0000 - mae: 10018.5713 - val_loss: 216624880.0000 - val_mae: 10216.7969 - learning_rate: 3.9063e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - loss: 217842352.0000 - mae: 10210.7002 - val_loss: 214732096.0000 - val_mae: 10161.9697 - learning_rate: 3.9063e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867us/step - loss: 212716528.0000 - mae: 10056.2549 - val_loss: 215590352.0000 - val_mae: 10128.3818 - learning_rate: 3.9063e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855us/step - loss: 214374672.0000 - mae: 10102.7314 - val_loss: 219473920.0000 - val_mae: 10296.8115 - learning_rate: 3.9063e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 215318528.0000 - mae: 10144.4717 - val_loss: 217787552.0000 - val_mae: 10278.2148 - learning_rate: 1.9531e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 210431680.0000 - mae: 10009.4326 - val_loss: 214771280.0000 - val_mae: 10150.9785 - learning_rate: 1.9531e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 211340528.0000 - mae: 10047.7793 - val_loss: 213153904.0000 - val_mae: 10135.1016 - learning_rate: 1.9531e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 215746640.0000 - mae: 10173.0488 - val_loss: 217134336.0000 - val_mae: 10196.1543 - learning_rate: 1.9531e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871us/step - loss: 215653088.0000 - mae: 10181.0234 - val_loss: 214117168.0000 - val_mae: 10149.3496 - learning_rate: 1.9531e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - loss: 217995264.0000 - mae: 10265.4375 - val_loss: 220481936.0000 - val_mae: 10303.4004 - learning_rate: 1.9531e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 214744080.0000 - mae: 10127.6240 - val_loss: 209079952.0000 - val_mae: 10067.1543 - learning_rate: 9.7656e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918us/step - loss: 213309200.0000 - mae: 10109.9971 - val_loss: 211052256.0000 - val_mae: 10027.1680 - learning_rate: 9.7656e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 214738864.0000 - mae: 10156.4258 - val_loss: 210320144.0000 - val_mae: 10006.5742 - learning_rate: 9.7656e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 212217776.0000 - mae: 10068.0352 - val_loss: 211084320.0000 - val_mae: 10035.7295 - learning_rate: 9.7656e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 875us/step - loss: 218133200.0000 - mae: 10263.2578 - val_loss: 219296032.0000 - val_mae: 10240.0811 - learning_rate: 9.7656e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 214120160.0000 - mae: 10099.1250 - val_loss: 216508576.0000 - val_mae: 10149.3076 - learning_rate: 9.7656e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - loss: 215191664.0000 - mae: 10155.9561 - val_loss: 220207728.0000 - val_mae: 10281.5078 - learning_rate: 4.8828e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - loss: 213750688.0000 - mae: 10112.9814 - val_loss: 213902832.0000 - val_mae: 10142.7295 - learning_rate: 4.8828e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 218665808.0000 - mae: 10226.9600 - val_loss: 211734912.0000 - val_mae: 10085.2139 - learning_rate: 4.8828e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 216760800.0000 - mae: 10228.7607 - val_loss: 214067792.0000 - val_mae: 10157.0498 - learning_rate: 4.8828e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - loss: 220364656.0000 - mae: 10331.0879 - val_loss: 213309120.0000 - val_mae: 10057.8838 - learning_rate: 4.8828e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853us/step - loss: 211081344.0000 - mae: 10036.1123 - val_loss: 215721584.0000 - val_mae: 10158.1826 - learning_rate: 4.8828e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 908us/step - loss: 215920832.0000 - mae: 10161.5283 - val_loss: 214310272.0000 - val_mae: 10068.4639 - learning_rate: 2.4414e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - loss: 215995488.0000 - mae: 10183.7100 - val_loss: 218816208.0000 - val_mae: 10264.0947 - learning_rate: 2.4414e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 213883648.0000 - mae: 10119.0742 - val_loss: 216459024.0000 - val_mae: 10224.1670 - learning_rate: 2.4414e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 213570656.0000 - mae: 10118.5264 - val_loss: 217409488.0000 - val_mae: 10194.2227 - learning_rate: 2.4414e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 213052864.0000 - mae: 10104.1768 - val_loss: 214998688.0000 - val_mae: 10171.4219 - learning_rate: 2.4414e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000us/step - loss: 211359552.0000 - mae: 10040.1846 - val_loss: 215308000.0000 - val_mae: 10196.5029 - learning_rate: 2.4414e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 212789568.0000 - mae: 10070.1455 - val_loss: 219224864.0000 - val_mae: 10306.3027 - learning_rate: 1.2207e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - loss: 215328432.0000 - mae: 10178.0029 - val_loss: 207743968.0000 - val_mae: 9938.1084 - learning_rate: 1.2207e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step - loss: 218719616.0000 - mae: 10270.6738 - val_loss: 213711536.0000 - val_mae: 10064.0391 - learning_rate: 1.2207e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853us/step - loss: 213802624.0000 - mae: 10113.7373 - val_loss: 214128592.0000 - val_mae: 10172.2070 - learning_rate: 1.2207e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 909us/step - loss: 214873184.0000 - mae: 10158.5420 - val_loss: 223395728.0000 - val_mae: 10385.7725 - learning_rate: 1.2207e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 213934224.0000 - mae: 10144.7422 - val_loss: 213015216.0000 - val_mae: 10145.6465 - learning_rate: 1.2207e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 217906224.0000 - mae: 10241.3779 - val_loss: 217302160.0000 - val_mae: 10161.5391 - learning_rate: 1.2207e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - loss: 213388192.0000 - mae: 10112.5166 - val_loss: 219518704.0000 - val_mae: 10317.8252 - learning_rate: 1.2207e-05\n",
      "Epoch 108/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 894us/step - loss: 217791808.0000 - mae: 10224.8613 - val_loss: 223305248.0000 - val_mae: 10358.2822 - learning_rate: 6.1035e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step - loss: 215026384.0000 - mae: 10133.1240 - val_loss: 210102368.0000 - val_mae: 9978.9414 - learning_rate: 6.1035e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - loss: 218081712.0000 - mae: 10245.1406 - val_loss: 214487456.0000 - val_mae: 10160.1807 - learning_rate: 6.1035e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 215665888.0000 - mae: 10177.9463 - val_loss: 213228672.0000 - val_mae: 10105.2744 - learning_rate: 6.1035e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874us/step - loss: 218852352.0000 - mae: 10278.2812 - val_loss: 217815440.0000 - val_mae: 10215.6270 - learning_rate: 6.1035e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 214751456.0000 - mae: 10134.0615 - val_loss: 218078560.0000 - val_mae: 10209.6064 - learning_rate: 6.1035e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 215269376.0000 - mae: 10136.8613 - val_loss: 218446912.0000 - val_mae: 10260.7080 - learning_rate: 3.0518e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step - loss: 217076112.0000 - mae: 10207.0312 - val_loss: 220222768.0000 - val_mae: 10374.4727 - learning_rate: 3.0518e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 215889728.0000 - mae: 10196.2461 - val_loss: 213113872.0000 - val_mae: 10075.5752 - learning_rate: 3.0518e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 217196096.0000 - mae: 10241.8994 - val_loss: 218081392.0000 - val_mae: 10224.1572 - learning_rate: 3.0518e-06\n",
      "Epoch 118/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887us/step - loss: 213404064.0000 - mae: 10122.7637 - val_loss: 215182544.0000 - val_mae: 10139.9297 - learning_rate: 3.0518e-06\n",
      "Epoch 119/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 215021360.0000 - mae: 10131.6709 - val_loss: 213564736.0000 - val_mae: 10109.5410 - learning_rate: 3.0518e-06\n",
      "Epoch 120/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - loss: 213064176.0000 - mae: 10117.2969 - val_loss: 219769808.0000 - val_mae: 10291.0977 - learning_rate: 1.5259e-06\n",
      "Epoch 121/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 935us/step - loss: 215821104.0000 - mae: 10163.1035 - val_loss: 214786464.0000 - val_mae: 10146.2686 - learning_rate: 1.5259e-06\n",
      "Epoch 122/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - loss: 218149312.0000 - mae: 10238.8906 - val_loss: 215435440.0000 - val_mae: 10128.6367 - learning_rate: 1.5259e-06\n",
      "Epoch 123/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step - loss: 214388352.0000 - mae: 10113.1475 - val_loss: 217160384.0000 - val_mae: 10221.8340 - learning_rate: 1.5259e-06\n",
      "Epoch 124/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888us/step - loss: 214133024.0000 - mae: 10136.1377 - val_loss: 216336784.0000 - val_mae: 10150.5664 - learning_rate: 1.5259e-06\n",
      "Epoch 125/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step - loss: 214266368.0000 - mae: 10141.9824 - val_loss: 220084384.0000 - val_mae: 10348.1094 - learning_rate: 1.5259e-06\n",
      "Epoch 126/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 906us/step - loss: 215478864.0000 - mae: 10192.3545 - val_loss: 224220496.0000 - val_mae: 10407.1357 - learning_rate: 1.0000e-06\n",
      "Epoch 127/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 216025088.0000 - mae: 10182.1475 - val_loss: 212614112.0000 - val_mae: 10119.0098 - learning_rate: 1.0000e-06\n",
      "Epoch 128/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step - loss: 213956560.0000 - mae: 10115.6992 - val_loss: 212431136.0000 - val_mae: 10062.4736 - learning_rate: 1.0000e-06\n",
      "Epoch 129/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 209300560.0000 - mae: 9984.0332 - val_loss: 220055600.0000 - val_mae: 10320.3477 - learning_rate: 1.0000e-06\n",
      "Epoch 130/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895us/step - loss: 220985600.0000 - mae: 10317.1074 - val_loss: 220112736.0000 - val_mae: 10302.2822 - learning_rate: 1.0000e-06\n",
      "Epoch 131/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 216584032.0000 - mae: 10231.7402 - val_loss: 216314176.0000 - val_mae: 10135.6504 - learning_rate: 1.0000e-06\n",
      "Epoch 132/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step - loss: 214486560.0000 - mae: 10142.9590 - val_loss: 214469600.0000 - val_mae: 10109.8789 - learning_rate: 1.0000e-06\n",
      "Epoch 133/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - loss: 215062224.0000 - mae: 10154.6270 - val_loss: 216246624.0000 - val_mae: 10266.5156 - learning_rate: 1.0000e-06\n",
      "Epoch 134/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855us/step - loss: 215429584.0000 - mae: 10154.4248 - val_loss: 220483920.0000 - val_mae: 10308.3994 - learning_rate: 1.0000e-06\n",
      "Epoch 135/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 992us/step - loss: 214396864.0000 - mae: 10127.2471 - val_loss: 211491728.0000 - val_mae: 10037.3477 - learning_rate: 1.0000e-06\n",
      "Epoch 136/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874us/step - loss: 214776336.0000 - mae: 10146.8643 - val_loss: 217350080.0000 - val_mae: 10262.1729 - learning_rate: 1.0000e-06\n",
      "Epoch 137/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step - loss: 215048480.0000 - mae: 10158.7568 - val_loss: 221551968.0000 - val_mae: 10370.6699 - learning_rate: 1.0000e-06\n",
      "Epoch 138/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 870us/step - loss: 210843712.0000 - mae: 10018.0186 - val_loss: 215778512.0000 - val_mae: 10153.9307 - learning_rate: 1.0000e-06\n",
      "Epoch 139/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 900us/step - loss: 212784576.0000 - mae: 10087.9863 - val_loss: 211866208.0000 - val_mae: 10006.1377 - learning_rate: 1.0000e-06\n",
      "Epoch 140/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872us/step - loss: 215837216.0000 - mae: 10178.6875 - val_loss: 216789728.0000 - val_mae: 10214.5645 - learning_rate: 1.0000e-06\n",
      "Epoch 141/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step - loss: 213867616.0000 - mae: 10110.9160 - val_loss: 221364112.0000 - val_mae: 10352.2568 - learning_rate: 1.0000e-06\n",
      "Epoch 142/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 213022960.0000 - mae: 10080.9600 - val_loss: 213731328.0000 - val_mae: 10118.8330 - learning_rate: 1.0000e-06\n",
      "Epoch 143/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883us/step - loss: 215430032.0000 - mae: 10162.9150 - val_loss: 212727040.0000 - val_mae: 10079.6338 - learning_rate: 1.0000e-06\n",
      "Epoch 144/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841us/step - loss: 212254784.0000 - mae: 10070.8096 - val_loss: 208859488.0000 - val_mae: 10006.7998 - learning_rate: 1.0000e-06\n",
      "Epoch 145/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - loss: 215751440.0000 - mae: 10184.8447 - val_loss: 215875120.0000 - val_mae: 10178.7256 - learning_rate: 1.0000e-06\n",
      "Epoch 146/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 931us/step - loss: 214948752.0000 - mae: 10142.6738 - val_loss: 221911264.0000 - val_mae: 10316.9688 - learning_rate: 1.0000e-06\n",
      "Epoch 147/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 215230384.0000 - mae: 10136.5010 - val_loss: 218276512.0000 - val_mae: 10287.1299 - learning_rate: 1.0000e-06\n",
      "Epoch 148/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 213973584.0000 - mae: 10125.2041 - val_loss: 218209552.0000 - val_mae: 10277.4941 - learning_rate: 1.0000e-06\n",
      "Epoch 149/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 216372480.0000 - mae: 10192.1846 - val_loss: 218208912.0000 - val_mae: 10234.8184 - learning_rate: 1.0000e-06\n",
      "Epoch 150/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step - loss: 214786048.0000 - mae: 10156.0654 - val_loss: 218772432.0000 - val_mae: 10263.2334 - learning_rate: 1.0000e-06\n",
      "Epoch 151/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 218226768.0000 - mae: 10219.3965 - val_loss: 213992768.0000 - val_mae: 10139.3857 - learning_rate: 1.0000e-06\n",
      "Epoch 152/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918us/step - loss: 215763728.0000 - mae: 10165.3662 - val_loss: 213500384.0000 - val_mae: 10112.6787 - learning_rate: 1.0000e-06\n",
      "Epoch 153/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867us/step - loss: 219694048.0000 - mae: 10267.5225 - val_loss: 214274064.0000 - val_mae: 10152.4580 - learning_rate: 1.0000e-06\n",
      "Epoch 154/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - loss: 214652256.0000 - mae: 10116.0107 - val_loss: 208716112.0000 - val_mae: 9954.5342 - learning_rate: 1.0000e-06\n",
      "Epoch 155/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847us/step - loss: 214084080.0000 - mae: 10135.8652 - val_loss: 215289040.0000 - val_mae: 10150.2490 - learning_rate: 1.0000e-06\n",
      "Epoch 156/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 214875168.0000 - mae: 10138.0039 - val_loss: 219882832.0000 - val_mae: 10261.8135 - learning_rate: 1.0000e-06\n",
      "Epoch 157/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845us/step - loss: 212454016.0000 - mae: 10079.7041 - val_loss: 218497888.0000 - val_mae: 10232.7754 - learning_rate: 1.0000e-06\n",
      "Epoch 158/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 914us/step - loss: 213399536.0000 - mae: 10092.1123 - val_loss: 215617312.0000 - val_mae: 10174.9570 - learning_rate: 1.0000e-06\n",
      "Epoch 159/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847us/step - loss: 213444672.0000 - mae: 10102.5557 - val_loss: 207027712.0000 - val_mae: 9878.4219 - learning_rate: 1.0000e-06\n",
      "Epoch 160/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 209552512.0000 - mae: 9977.6963 - val_loss: 220271648.0000 - val_mae: 10298.1025 - learning_rate: 1.0000e-06\n",
      "Epoch 161/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 216260112.0000 - mae: 10179.1113 - val_loss: 215165072.0000 - val_mae: 10144.8105 - learning_rate: 1.0000e-06\n",
      "Epoch 162/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step - loss: 216673296.0000 - mae: 10206.3447 - val_loss: 210885072.0000 - val_mae: 10042.6172 - learning_rate: 1.0000e-06\n",
      "Epoch 163/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step - loss: 217625312.0000 - mae: 10202.8418 - val_loss: 216976656.0000 - val_mae: 10210.2070 - learning_rate: 1.0000e-06\n",
      "Epoch 164/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 215512640.0000 - mae: 10168.9600 - val_loss: 224236288.0000 - val_mae: 10467.5928 - learning_rate: 1.0000e-06\n",
      "Epoch 165/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891us/step - loss: 210926960.0000 - mae: 10062.9570 - val_loss: 219834192.0000 - val_mae: 10310.8750 - learning_rate: 1.0000e-06\n",
      "Epoch 166/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 928us/step - loss: 215361552.0000 - mae: 10169.4268 - val_loss: 213297984.0000 - val_mae: 10109.4346 - learning_rate: 1.0000e-06\n",
      "Epoch 167/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 917us/step - loss: 213914912.0000 - mae: 10119.8936 - val_loss: 218506992.0000 - val_mae: 10249.0146 - learning_rate: 1.0000e-06\n",
      "Epoch 168/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 216221760.0000 - mae: 10209.9209 - val_loss: 215693072.0000 - val_mae: 10174.2578 - learning_rate: 1.0000e-06\n",
      "Epoch 169/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872us/step - loss: 217609312.0000 - mae: 10202.1641 - val_loss: 215324432.0000 - val_mae: 10155.9609 - learning_rate: 1.0000e-06\n",
      "Epoch 170/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step - loss: 215366384.0000 - mae: 10147.0859 - val_loss: 220663712.0000 - val_mae: 10296.0088 - learning_rate: 1.0000e-06\n",
      "Epoch 171/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 898us/step - loss: 215269360.0000 - mae: 10156.0156 - val_loss: 213129024.0000 - val_mae: 10114.1299 - learning_rate: 1.0000e-06\n",
      "Epoch 172/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879us/step - loss: 217889760.0000 - mae: 10240.4375 - val_loss: 217941248.0000 - val_mae: 10234.3545 - learning_rate: 1.0000e-06\n",
      "Epoch 173/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 925us/step - loss: 216884432.0000 - mae: 10178.5020 - val_loss: 214418304.0000 - val_mae: 10159.7451 - learning_rate: 1.0000e-06\n",
      "Epoch 174/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step - loss: 214139616.0000 - mae: 10092.8516 - val_loss: 217158832.0000 - val_mae: 10196.3037 - learning_rate: 1.0000e-06\n",
      "Epoch 175/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step - loss: 216713264.0000 - mae: 10227.3740 - val_loss: 215356992.0000 - val_mae: 10164.2773 - learning_rate: 1.0000e-06\n",
      "Epoch 176/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - loss: 218401680.0000 - mae: 10244.4766 - val_loss: 219006832.0000 - val_mae: 10260.7256 - learning_rate: 1.0000e-06\n",
      "Epoch 177/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 215410016.0000 - mae: 10150.8799 - val_loss: 212037056.0000 - val_mae: 9998.0068 - learning_rate: 1.0000e-06\n",
      "Epoch 178/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step - loss: 217516384.0000 - mae: 10237.5459 - val_loss: 213706080.0000 - val_mae: 10108.4795 - learning_rate: 1.0000e-06\n",
      "Epoch 179/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830us/step - loss: 215085008.0000 - mae: 10158.9443 - val_loss: 208848448.0000 - val_mae: 9932.2627 - learning_rate: 1.0000e-06\n",
      "Epoch 180/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841us/step - loss: 218348416.0000 - mae: 10245.2480 - val_loss: 212073712.0000 - val_mae: 10041.6758 - learning_rate: 1.0000e-06\n",
      "Epoch 181/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 216576560.0000 - mae: 10186.2412 - val_loss: 217168304.0000 - val_mae: 10227.8936 - learning_rate: 1.0000e-06\n",
      "Epoch 182/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 219106496.0000 - mae: 10233.5107 - val_loss: 218303488.0000 - val_mae: 10223.5117 - learning_rate: 1.0000e-06\n",
      "Epoch 183/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 212987248.0000 - mae: 10107.5303 - val_loss: 218606000.0000 - val_mae: 10245.6582 - learning_rate: 1.0000e-06\n",
      "Epoch 184/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 214937344.0000 - mae: 10138.5586 - val_loss: 217806608.0000 - val_mae: 10253.5938 - learning_rate: 1.0000e-06\n",
      "Epoch 185/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step - loss: 207679072.0000 - mae: 9921.5820 - val_loss: 207131232.0000 - val_mae: 9890.3350 - learning_rate: 1.0000e-06\n",
      "Epoch 186/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step - loss: 217400768.0000 - mae: 10221.8525 - val_loss: 218457328.0000 - val_mae: 10241.5771 - learning_rate: 1.0000e-06\n",
      "Epoch 187/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 214266176.0000 - mae: 10174.0098 - val_loss: 212385472.0000 - val_mae: 10042.9102 - learning_rate: 1.0000e-06\n",
      "Epoch 188/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - loss: 214241856.0000 - mae: 10108.0381 - val_loss: 214010608.0000 - val_mae: 10089.7383 - learning_rate: 1.0000e-06\n",
      "Epoch 189/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - loss: 216338096.0000 - mae: 10202.0869 - val_loss: 210181792.0000 - val_mae: 10058.1924 - learning_rate: 1.0000e-06\n",
      "Epoch 190/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 218267616.0000 - mae: 10227.3955 - val_loss: 216563248.0000 - val_mae: 10187.8721 - learning_rate: 1.0000e-06\n",
      "Epoch 191/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 962us/step - loss: 211705584.0000 - mae: 10060.2891 - val_loss: 215253456.0000 - val_mae: 10193.8086 - learning_rate: 1.0000e-06\n",
      "Epoch 192/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step - loss: 216519248.0000 - mae: 10210.6592 - val_loss: 214783024.0000 - val_mae: 10165.3613 - learning_rate: 1.0000e-06\n",
      "Epoch 193/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 216171040.0000 - mae: 10147.7227 - val_loss: 216116720.0000 - val_mae: 10193.4150 - learning_rate: 1.0000e-06\n",
      "Epoch 194/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step - loss: 218466896.0000 - mae: 10245.5391 - val_loss: 223017024.0000 - val_mae: 10365.2217 - learning_rate: 1.0000e-06\n",
      "Epoch 195/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 929us/step - loss: 217865632.0000 - mae: 10237.1787 - val_loss: 209320384.0000 - val_mae: 10015.2285 - learning_rate: 1.0000e-06\n",
      "Epoch 196/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 216854496.0000 - mae: 10220.2520 - val_loss: 218087456.0000 - val_mae: 10237.7451 - learning_rate: 1.0000e-06\n",
      "Epoch 197/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step - loss: 213143248.0000 - mae: 10098.0400 - val_loss: 213401056.0000 - val_mae: 10153.3467 - learning_rate: 1.0000e-06\n",
      "Epoch 198/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 218936464.0000 - mae: 10262.3105 - val_loss: 219924800.0000 - val_mae: 10293.4941 - learning_rate: 1.0000e-06\n",
      "Epoch 199/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 215754176.0000 - mae: 10167.1680 - val_loss: 210832720.0000 - val_mae: 9954.0986 - learning_rate: 1.0000e-06\n",
      "Epoch 200/200\n",
      "\u001b[1m879/879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 214948672.0000 - mae: 10147.9531 - val_loss: 213613616.0000 - val_mae: 10097.3857 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Choose the metric to monitor\n",
    "    factor=0.5,          # Factor by which to reduce the learning rate\n",
    "    patience=6,          # Number of epochs with no improvement before reducing\n",
    "    min_lr=1e-6          # Minimum learning rate\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../models/Final_training/third_model_5.keras',             # File to save the best model\n",
    "    monitor='val_accuracy',       # Metric to monitor for improvement\n",
    "    mode='max',                   # Mode 'max' for accuracy (since higher is better)\n",
    "    save_best_only=True,          # Save only when there is an improvement\n",
    "    verbose=1,                    # Print message when saving\n",
    "                    \n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    epochs=200,\n",
    "    callbacks=[reduce_lr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/model_1.h5\")  # Saves the model as an HDF5 file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge_ENS_env_CFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
